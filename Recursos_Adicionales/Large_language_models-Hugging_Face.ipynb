{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a los Large Language Models (LLMs) usando Python\n",
    "\n",
<<<<<<< HEAD
    "Autor: Manuel Eugenio Morocho Cayamcela, PhD\n",
    "\n",
=======
>>>>>>> d9c17a2d8048f1de958fe89c53f95ef2c76d35d5
    "## Objetivo:\n",
    "El objetivo de este taller es introducir a los estudiantes a los conceptos básicos de los large language models (LLMs) y proporcionarles una experiencia práctica utilizando bibliotecas de Hugging Face para trabajar con modelos preentrenados de lenguaje natural.\n",
    "\n",
    "## Contenido:\n",
    "1. Introducción a los Large Language Models (LLMs)\n",
    "2. Configuración del entorno\n",
    "3. Ejemplos prácticos\n",
    "4. Ejercicio\n",
    "\n",
    "## 1. Introducción a los Large Language Models (LLMs)\n",
    "\n",
    "Los large language models (LLMs) son modelos de inteligencia artificial que han sido entrenados en grandes cantidades de texto para comprender y generar lenguaje natural. Estos modelos pueden realizar una variedad de tareas de procesamiento de lenguaje natural (NLP) como traducción, resumen, generación de texto, respuesta a preguntas, entre otros.\n",
    "\n",
    "## 2. Configuración del entorno\n",
    "\n",
    "Para este taller, utilizaremos la biblioteca `transformers` de Hugging Face. Primero, necesitamos instalar las bibliotecas necesarias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.15.3-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.5.15-cp311-cp311-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Using cached tokenizers-0.19.1-cp311-cp311-macosx_10_12_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.3-cp311-cp311-macosx_10_12_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages (from transformers) (4.66.4)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.0->transformers)\n",
      "  Using cached fsspec-2024.6.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages (from requests->transformers) (2024.6.2)\n",
      "Using cached transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "Using cached huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "Using cached regex-2024.5.15-cp311-cp311-macosx_10_9_x86_64.whl (281 kB)\n",
      "Using cached safetensors-0.4.3-cp311-cp311-macosx_10_12_x86_64.whl (415 kB)\n",
      "Using cached tokenizers-0.19.1-cp311-cp311-macosx_10_12_x86_64.whl (2.5 MB)\n",
      "Downloading filelock-3.15.3-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2024.6.0-py3-none-any.whl (176 kB)\n",
      "Installing collected packages: safetensors, regex, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.15.3 fsspec-2024.6.0 huggingface-hub-0.23.4 regex-2024.5.15 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.41.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting torch\n",
      "  Using cached torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages (from torch) (3.15.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.12.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Collecting mpmath<1.4.0,>=1.1.0 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached torch-2.2.2-cp311-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "Using cached sympy-1.12.1-py3-none-any.whl (5.7 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, torch\n",
      "Successfully installed mpmath-1.3.0 sympy-1.12.1 torch-2.2.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting tf-keras\n",
      "  Using cached tf_keras-2.16.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tensorflow<2.17,>=2.16 (from tf-keras)\n",
      "  Using cached tensorflow-2.16.1-cp311-cp311-macosx_10_15_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Using cached gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Using cached h5py-3.11.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-macosx_10_9_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Using cached ml_dtypes-0.3.2-cp311-cp311-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (24.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Using cached protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Using cached wrapt-1.16.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Using cached grpcio-1.64.1-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.3 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Using cached tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Using cached keras-3.3.3-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.37.0-cp311-cp311-macosx_10_14_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages (from tensorflow<2.17,>=2.16->tf-keras) (1.26.4)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Using cached wheel-0.43.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting rich (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Using cached optree-0.11.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (45 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.17,>=2.16->tf-keras) (2024.6.2)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Using cached Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-macosx_10_9_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Using cached werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tf-keras) (2.1.5)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tf-keras)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n",
      "Using cached tensorflow-2.16.1-cp311-cp311-macosx_10_15_x86_64.whl (259.6 MB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached grpcio-1.64.1-cp311-cp311-macosx_10_9_universal2.whl (10.4 MB)\n",
      "Using cached h5py-3.11.0-cp311-cp311-macosx_10_9_x86_64.whl (3.5 MB)\n",
      "Using cached keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-macosx_10_9_x86_64.whl (26.5 MB)\n",
      "Using cached ml_dtypes-0.3.2-cp311-cp311-macosx_10_9_universal2.whl (389 kB)\n",
      "Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Using cached protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "Using cached tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.37.0-cp311-cp311-macosx_10_14_x86_64.whl (2.5 MB)\n",
      "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached wrapt-1.16.0-cp311-cp311-macosx_10_9_x86_64.whl (37 kB)\n",
      "Using cached Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-macosx_10_9_x86_64.whl (4.8 MB)\n",
      "Using cached werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "Using cached wheel-0.43.0-py3-none-any.whl (65 kB)\n",
      "Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Using cached optree-0.11.0-cp311-cp311-macosx_10_9_x86_64.whl (296 kB)\n",
      "Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, opt-einsum, ml-dtypes, mdurl, markdown, h5py, grpcio, google-pasta, gast, absl-py, tensorboard, markdown-it-py, astunparse, rich, keras, tensorflow, tf-keras\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.5.4 google-pasta-0.2.0 grpcio-1.64.1 h5py-3.11.0 keras-3.3.3 libclang-18.1.1 markdown-3.6 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.3.2 namex-0.0.8 opt-einsum-3.3.0 optree-0.11.0 protobuf-4.25.3 rich-13.7.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-io-gcs-filesystem-0.37.0 termcolor-2.4.0 tf-keras-2.16.0 werkzeug-3.0.3 wheel-0.43.0 wrapt-1.16.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Instalación de la biblioteca transformers y torch\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos `pipeline` de la librería `transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-22 06:18:15.791382: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ejemplos prácticos de carga y uso de modelos preentrenados\n",
    "\n",
    "### 3.1 Generación de texto (`text-generation`)\n",
    "\n",
    "Vamos a cargar un modelo preentrenado de Hugging Face y usarlo para generar texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Había una vez árejora con las luna una español como por parte la muy de que esta hablar.\n",
      "\n",
      "Nosquiel como hablar de\n"
     ]
    }
   ],
   "source": [
    "# Cargamos un pipeline de generación de texto\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "# Generamos texto con el modelo cargado\n",
    "text = generator(\"Había una vez\", max_length=50, num_return_sequences=1)\n",
    "print(text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora usamos el modelo para la generación de texto mediante prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: In the future, AI will\n",
      "Generated Text: In the future, AI will be able to learn, and then learn to learn. Then, our human brain uses that knowledge to design AI. The future is also going to be much smarter.\n",
      "\n",
      "The question is, would you like to start\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The secret to happiness is\n",
      "Generated Text: The secret to happiness is not to think on your feet. It is to put aside ideas that are too often the result of selfishness. In my home life for a long time my friends and I were convinced the solution was to give ourselves the freedom\n",
      "\n",
      "Prompt: The quick brown fox\n",
      "Generated Text: The quick brown fox has always been a very big pet, but I think it's time that other pets, especially dogs, started getting smaller and shorter, and that we really want them to become larger and bigger.\n",
      "\n",
      "It's an intriguing idea\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de generación de texto con diferentes prompts\n",
    "prompts = [\n",
    "    \"In the future, AI will\",\n",
    "    \"The secret to happiness is\",\n",
    "    \"The quick brown fox\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    text = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated Text: {text[0]['generated_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Respuesta a preguntas (`question-answering`)\n",
    "\n",
    "Podemos usar un pipeline de respuesta a preguntas para encontrar respuestas dentro de un contexto proporcionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos un pipeline de respuesta a preguntas\n",
    "qa_pipeline = pipeline('question-answering')\n",
    "\n",
    "# Definimos el contexto y la pregunta\n",
    "context = \"Eres un profesor de matemática especializado en describir conceptos básicos para estudiantes de primaria.\"\n",
    "question = \"¿Cómo ajusto datos en un modelo de regresión lineal?\"\n",
    "\n",
    "# Obtenemos la respuesta\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "print(f\"Pregunta: {question}\")\n",
    "print(f\"Respuesta: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Resumen de texto\n",
    "\n",
    "Usa un modelo preentrenado para generar un resumen de un texto largo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen:\n",
      " La inteligencia artificial (IA) se refiere a la simulación of the human intelligence . La característica ideal de la IAI is su capacidad for racionalizar y tomar acciones\n"
     ]
    }
   ],
   "source": [
    "# Cargamos un pipeline de resumen\n",
    "summarizer = pipeline('summarization')\n",
    "\n",
    "# Texto largo para resumir\n",
    "long_text = \"\"\"\n",
    "La inteligencia artificial (IA) se refiere a la simulación de la inteligencia humana en máquinas que están programadas para pensar como humanos y \n",
    "imitar sus acciones. El término también se puede aplicar a cualquier máquina que exhiba rasgos asociados con una mente humana como el aprendizaje y \n",
    "la resolución de problemas. La característica ideal de la inteligencia artificial es su capacidad para racionalizar y tomar acciones que tengan \n",
    "la mejor posibilidad de alcanzar un objetivo específico. Un subconjunto de la inteligencia artificial es el aprendizaje automático, que se refiere a \n",
    "la idea de que los sistemas informáticos pueden aprender de datos, identificar patrones y tomar decisiones con una mínima intervención humana.\n",
    "\"\"\"\n",
    "\n",
    "# Generamos el resumen\n",
    "summary = summarizer(long_text, max_length=50, min_length=25, do_sample=False)\n",
    "print(\"Resumen:\")\n",
    "print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Traducción de texto (`translation`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos un modelo traductor preentrenado de inglés a español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /Users/eugenio/Library/CloudStorage/OneDrive-Personal/UHE/2024 Maestría IA Aplicada/Notebooks_PercepcionComputacional/.venv/lib/python3.11/site-packages (0.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eugenio/Library/CloudStorage/OneDrive-Personal/UHE/2024 Maestría IA Aplicada/Notebooks_PercepcionComputacional/.venv/lib/python3.11/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original: Persistent homology is a method for computing topological features of a space at different spatial resolutions. More persistent features are detected over a wide range of spatial scales and are deemed more likely to represent true features of the underlying space rather than artifacts of sampling, noise, or particular choice of parameters\n",
      "Traducción: La homología persistente es un método para calcular las características topológicas de un espacio en diferentes resoluciones espaciales. Se detectan características más persistentes en una amplia gama de escalas espaciales y se considera más probable que representen características verdaderas del espacio subyacente en lugar de artefactos de muestreo, ruido o elección particular de parámetros.\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n",
    "import sentencepiece\n",
    "\n",
    "# Cargamos un pipeline de traducción especificando el modelo\n",
    "translation_pipeline = pipeline('translation', model='Helsinki-NLP/opus-mt-en-es')\n",
    "\n",
    "# Definimos el texto a traducir\n",
    "text = \"Persistent homology is a method for computing topological features of a space at different spatial resolutions. More persistent features are detected over a wide range of spatial scales and are deemed more likely to represent true features of the underlying space rather than artifacts of sampling, noise, or particular choice of parameters\"\n",
    "\n",
    "# Obtenemos la traducción.\n",
    "result = translation_pipeline(text)\n",
    "print(f\"Texto original: {text}\")\n",
    "print(f\"Traducción: {result[0]['translation_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Análisis de sentimientos (`sentiment-analysis`)\n",
    "\n",
    "Ahora usaremos un modelo de análisis de sentimientos para predecir si el texto es positivo o negativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/Users/eugenio/Library/CloudStorage/OneDrive-Personal/UHE/2024 Maestría IA Aplicada/Notebooks_PercepcionComputacional/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Result:\n",
      "Label: POSITIVE, Score: 0.999862551689148\n"
     ]
    }
   ],
   "source": [
    "# Load the sentiment analysis pipeline\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Text to analyze sentiment\n",
    "text = \"I'm feeling very happy today because I just got promoted at work! It's such a great day and I'm excited about the new opportunities ahead. However, I'm a bit nervous about the new responsibilities, but overall, I think this is a positive change.\"\n",
    "\n",
    "# Perform sentiment analysis\n",
    "sentiment_result = sentiment_analyzer(text)\n",
    "\n",
    "# Print the sentiment result\n",
    "print(\"Sentiment Analysis Result:\")\n",
    "for result in sentiment_result:\n",
    "    print(f\"Label: {result['label']}, Score: {result['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Preguntando a una tabla (`table-question-answering`)\n",
    "\n",
    "Ahora vamos a hacer preguntas a un DataFrame de Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "Table:\n",
      "    CustomerID  Gender Age Annual Income (k$) Spending Score (1-100)\n",
      "0            1    Male  19                 15                     39\n",
      "1            2    Male  21                 15                     81\n",
      "2            3  Female  20                 16                      6\n",
      "3            4  Female  23                 16                     77\n",
      "4            5  Female  31                 17                     40\n",
      "..         ...     ...  ..                ...                    ...\n",
      "195        196  Female  35                120                     79\n",
      "196        197  Female  45                126                     28\n",
      "197        198    Male  32                126                     74\n",
      "198        199    Male  32                137                     18\n",
      "199        200    Male  30                137                     83\n",
      "\n",
      "[200 rows x 5 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages/transformers/models/tapas/tokenization_tapas.py:2674: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text = normalize_for_match(row[col_index].text)\n",
      "/Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/.venv/lib/python3.11/site-packages/transformers/models/tapas/tokenization_tapas.py:1473: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  cell = row[col_index]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How can I segment the clients based on their Annual Income (k$) and Spending Score (1-100)?\n",
      "Answer: SUM > 1, 2, 3, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96\n",
      "Question: I want to segment the clients based on their Annual Income (k$) and Spending Score (1-100) so I can target them better. How can I do that?\n",
      "Answer: AVERAGE > \n",
      "Question: What kind of marketing campaigns can I design to increase the Spending Score (1-100) of the customers that have a high Annual Income (k$) but low Spending Score (1-100)?\n",
      "Answer: SUM > 1, 2, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load a table-question-answering pipeline\n",
    "table_qa = pipeline(\"table-question-answering\", model=\"google/tapas-base-finetuned-wtq\")\n",
    "\n",
    "# Create a table as a pandas DataFrame\n",
    "data = {\n",
    "    \"Name\": [\"Gaby\", \"May\", \"Andre\"],\n",
    "    \"Age\": [\"23\", \"26\", \"23\"],  # Ensure all entries are strings\n",
    "    \"Interest\": [\"Persistent Homology\", \"Numerical Analysis\", \"Functinal Analysis\"]\n",
    "}\n",
    "\n",
    "# Imprimimos el tipo de variable de la base de datos\n",
    "print(type(data))\n",
    "\n",
    "# Carga la base de datos 'mall_customers.csv'\n",
    "data = pd.read_csv(\"/Users/eugenio/Library/CloudStorage/OneDrive-Personal/UIDE/2024 Maestría Ciencia de Datos y Máquinas de Aprendizaje mención IA/Contenidos/Notebooks_DataMining/Bases de Datos/mall_customers.csv\")\n",
    "\n",
    "# Convertimos la base de datos a diccionario, asegurando que todos los datos sean cadenas de texto\n",
    "data = data.astype(str).to_dict(orient=\"list\")\n",
    "\n",
    "# Crea un DataFrame con la base de datos, asegurándose de que todos los datos sean cadenas de texto\n",
    "table = pd.DataFrame(data)\n",
    "\n",
    "# Ensure the table is in the correct format\n",
    "print(\"Table:\")\n",
    "print(table)\n",
    "\n",
    "# Question about the table\n",
    "question = \"How can I segment the clients based on their Annual Income (k$) and Spending Score (1-100)?\"\n",
    "question_2 = \"I want to segment the clients based on their Annual Income (k$) and Spending Score (1-100) so I can target them better. How can I do that?\"\n",
    "question_3 = \"What kind of marketing campaigns can I design to increase the Spending Score (1-100) of the customers that have a high Annual Income (k$) but low Spending Score (1-100)?\"\n",
    "\n",
    "# Generate the answer\n",
    "answer = table_qa(table=table, query=question)\n",
    "answer_2 = table_qa(table=table, query=question_2)\n",
    "answer_3 = table_qa(table=table, query=question_3)\n",
    "\n",
    "# Print the question and the answer\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer['answer'])\n",
    "\n",
    "print(\"Question:\", question_2)\n",
    "print(\"Answer:\", answer_2['answer'])\n",
    "\n",
    "print(\"Question:\", question_3)\n",
    "print(\"Answer:\", answer_3['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otros modelos disponibles:\n",
    "\n",
    "- 'audio-classification'\n",
    "- 'automatic-speech-recognition'\n",
    "- 'conversational'\n",
    "- 'depth-estimation'\n",
    "- 'document-question-answering'\n",
    "- 'feature-extraction'\n",
    "- 'fill-mask'\n",
    "- 'image-classification'\n",
    "- 'image-feature-extraction'\n",
    "- 'image-segmentation'\n",
    "- 'image-to-image'\n",
    "- 'image-to-text'\n",
    "- 'mask-generation'\n",
    "- 'ner', 'object-detection'\n",
    "- 'question-answering'\n",
    "- 'sentiment-analysis'\n",
    "- 'summarization'\n",
    "- 'table-question-answering'\n",
    "- 'text-classification'\n",
    "- 'text-generation'\n",
    "- 'text-to-audio'\n",
    "- 'text-to-speech'\n",
    "- 'text2text-generation'\n",
    "- 'token-classification'\n",
    "- 'translation'\n",
    "- 'video-classification'\n",
    "- 'visual-question-answering'\n",
    "- 'vqa'\n",
    "- 'zero-shot-audio-classification'\n",
    "- 'zero-shot-classification'\n",
    "- 'zero-shot-image-classification'\n",
    "- 'zero-shot-object-detection'\n",
    "- 'translation_XX_to_YY'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
